{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train and Resume .ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/Gauravv97/Tensorflow-deeplabV3-TrainSaveResume-on-Android/blob/master/Train_and_Resume_.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "3iAwfzqB7W1h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "from os.path import expanduser\n",
        "import tensorflow as tf \n",
        "from io import BytesIO\n",
        "import tarfile\n",
        "import tempfile\n",
        "from six.moves import urllib\n",
        "\n",
        "home = expanduser(\"~\")\n",
        "os.chdir(home)\n",
        "if not (os.path.exists('''models''')):\n",
        "  !git clone https://github.com/tensorflow/models.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iqVAjHlz5_fN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZkeBQ7Pb7Ypm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.chdir(home+'''/models/research/''')\n",
        "!python slim/setup.py build\n",
        "!python slim/setup.py install"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WCL3LUpi7qQo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.chdir(home+\"/models/research\")\n",
        "!export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim\n",
        "!cp -r slim/* deeplab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q9VNC1Lu8vEt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.chdir(home+\"/models/research/deeplab/datasets\")\n",
        "!sh download_and_convert_voc2012.sh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZNdvOAcF7uTG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/models/research/deeplab/datasets/pascal_voc_seg/init_models\n",
        "!mkdir -p /content/models/research/deeplab/datasets/pascal_voc_seg/exp/train_on_trainval_set/train\n",
        "!mkdir -p /content/models/research/deeplab/datasets/pascal_voc_seg/exp/train_on_trainval_set/eval\n",
        "!mkdir -p /content/models/research/deeplab/datasets/pascal_voc_seg/exp/train_on_trainval_set/vis\n",
        "!mkdir -p /content/models/research/deeplab/datasets/pascal_voc_seg/exp/train_on_trainval_set/export\n",
        "os.chdir('/content/models/research/deeplab/datasets/pascal_voc_seg/init_models')\n",
        "!wget -nd -c \"http://download.tensorflow.org/models/deeplabv3_pascal_train_aug_2018_01_04.tar.gz\"\n",
        "!tar -xf \"deeplabv3_pascal_train_aug_2018_01_04.tar.gz\"\n",
        "os.chdir('/content/models/research/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OfDC8eYIaJTu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q5FRdUGpZd0M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once in a notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "\n",
        "folder_metadata = {\n",
        "    'title' : 'train',\n",
        "    # The mimetype defines this new file as a folder, so don't change this.\n",
        "    'mimeType' : 'application/vnd.google-apps.folder'\n",
        "}\n",
        "file_list = drive.ListFile({'q': \"'root' in parents and trashed=false\"}).GetList()\n",
        "flag=0\n",
        "for file1 in file_list:\n",
        "    if file1['title'] == 'train':\n",
        "        folder_id = file1['id']\n",
        "        flag=1\n",
        "if flag==0:\n",
        "  folder = drive.CreateFile(folder_metadata)\n",
        "  folder.Upload()\n",
        "  folder_title = folder['title']\n",
        "  folder_id = folder['id']\n",
        "\n",
        "\n",
        "\n",
        "files = [f for f in os.listdir('.') if os.path.isfile(f)]\n",
        "for f in files:\n",
        "  file = drive.CreateFile({\"parents\": [{\"kind\": \"drive#fileLink\", \"id\": folder_id}]})\n",
        "  print(\"uploading \"+f)\n",
        "  file.SetContentFile(f)\n",
        "  file.Upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hvjc2m_AJvUs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once in a notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "folder_metadata = {\n",
        "    'title' : 'train',\n",
        "    # The mimetype defines this new file as a folder, so don't change this.\n",
        "    'mimeType' : 'application/vnd.google-apps.folder'\n",
        "}\n",
        "file_list = drive.ListFile({'q': \"'root' in parents and trashed=false\"}).GetList()\n",
        "flag=0\n",
        "for file1 in file_list:\n",
        "    if file1['title'] == 'train':\n",
        "        folder_id = file1['id']\n",
        "        flag=1\n",
        "if flag==0:\n",
        "  folder = drive.CreateFile(folder_metadata)\n",
        "  folder.Upload()\n",
        "  folder_title = folder['title']\n",
        "  folder_id = folder['id']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dtnta4bdo4yv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''Remove all previous checkpoint and train files if exists '''\n",
        "!rm -r /content/models/research/deeplab/datasets/pascal_voc_seg/exp/train_on_trainval_set/train/*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bGPEvTtW24cU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls /content/models/research/deeplab/datasets/pascal_voc_seg/exp/train_on_trainval_set/train/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1-bC1iNHnH3T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.chdir('/content/models/research/deeplab/datasets/pascal_voc_seg/exp/train_on_trainval_set/train')\n",
        "file_list = drive.ListFile({'q': \"'{}' in parents and trashed=false\".format(folder_id)}).GetList()\n",
        "for file1 in sorted(file_list, key = lambda x: x['title']):\n",
        "    file1.GetContentFile(file1['title'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iliYH-TC70my",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python /content/models/research/deeplab/train.py \\\n",
        "  --logtostderr \\\n",
        "  --train_split=\"trainval\" \\\n",
        "  --model_variant=\"xception_65\" \\\n",
        "  --atrous_rates=6 \\\n",
        "  --atrous_rates=12 \\\n",
        "  --atrous_rates=18 \\\n",
        "  --output_stride=16 \\\n",
        "  --decoder_output_stride=4 \\\n",
        "  --train_crop_size=1025 \\\n",
        "  --train_crop_size=1025 \\\n",
        "  --train_batch_size=1 \\\n",
        "  --training_number_of_steps=20 \\\n",
        "  --fine_tune_batch_norm=false \\\n",
        "  --tf_initial_checkpoint=\"/content/models/research/deeplab/datasets/pascal_voc_seg/init_models/deeplabv3_pascal_train_aug/model.ckpt\" \\\n",
        "  --train_logdir=\"/content/models/research/deeplab/datasets/pascal_voc_seg/exp/train_on_trainval_set/train\" \\\n",
        "  --dataset_dir=\"/content/models/research/deeplab/datasets/pascal_voc_seg/tfrecord\"\n",
        "\n",
        "\n",
        "\n",
        "''' Delete All old train files in Google Drive and upload new ones'''\n",
        "os.chdir('/content/models/research/deeplab/datasets/pascal_voc_seg/exp/train_on_trainval_set/train')\n",
        "file_list = drive.ListFile({'q': \"'{}' in parents and trashed=false\".format(folder_id)}).GetList()\n",
        "for file1 in sorted(file_list, key = lambda x: x['title']):\n",
        "    file1.Delete() \n",
        "files = [f for f in os.listdir('.') if os.path.isfile(f)]\n",
        "for f in files:\n",
        "  file = drive.CreateFile({\"parents\": [{\"kind\": \"drive#fileLink\", \"id\": folder_id}]})\n",
        "  print(\"uploading \"+f)\n",
        "  file.SetContentFile(f)\n",
        "  file.Upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tkomOtm23gcm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls /content/models/research/deeplab/datasets/pascal_voc_seg/exp/train_on_trainval_set/train/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k2ulUWh3y_oc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4eawW8GZ4I01",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.chdir('/content/models/research/deeplab')\n",
        "!python export_model.py \\\n",
        "  --logtostderr \\\n",
        "  --checkpoint_path=\"/content/models/research/deeplab/datasets/pascal_voc_seg/exp/train_on_trainval_set/train/model.ckpt-200\" \\\n",
        "  --export_path=\"/content/models/research/deeplab/frozen_inference_graph.pb\" \\\n",
        "  --model_variant=\"xception_65\" \\\n",
        "  --atrous_rates=6 \\\n",
        "  --atrous_rates=12 \\\n",
        "  --atrous_rates=18 \\\n",
        "  --output_stride=16 \\\n",
        "  --decoder_output_stride=4 \\\n",
        "  --num_classes=21 \\\n",
        "  --crop_size=1025 \\\n",
        "  --crop_size=1025 \\\n",
        "  --inference_scales=1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "07nX6FxlPxTT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "from io import BytesIO\n",
        "import tarfile\n",
        "import tempfile\n",
        "import tensorflow as tf\n",
        "from six.moves import urllib\n",
        "\n",
        "from matplotlib import gridspec\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "reNghXVVMuh-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "SAMPLE_IMAGE = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gz9JY6KYPIs-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "INPUT_TENSOR_NAME = 'ImageTensor:0'\n",
        "OUTPUT_TENSOR_NAME = 'SemanticPredictions:0'\n",
        "INPUT_SIZE = 1025\n",
        "FROZEN_GRAPH_NAME = 'frozen_inference_graph'\n",
        "\n",
        "graph = tf.Graph()\n",
        "\n",
        "graph_def = None\n",
        "\n",
        "file=open(\"/content/models/research/deeplab/frozen_inference_graph.pb\",'r')\n",
        "graph_def = tf.GraphDef.FromString(file.read())\n",
        "if graph_def is None:\n",
        "      raise RuntimeError('Cannot find inference graph.')\n",
        "with graph.as_default():\n",
        "      tf.import_graph_def(graph_def, name='')\n",
        "sess = tf.Session(graph=graph)\n",
        "\n",
        "\n",
        "def run(image):\n",
        "    \"\"\"Runs inference on a single image.\n",
        "\n",
        "    Args:\n",
        "      image: A PIL.Image object, raw input image.\n",
        "\n",
        "    Returns:\n",
        "      resized_image: RGB image resized from original input image.\n",
        "      seg_map: Segmentation map of `resized_image`.\n",
        "    \"\"\"\n",
        "    width, height = image.size\n",
        "    resize_ratio = 1.0 * INPUT_SIZE / max(width, height)\n",
        "    target_size = (int(resize_ratio * width), int(resize_ratio * height))\n",
        "    resized_image = image.convert('RGB').resize(target_size, Image.ANTIALIAS)\n",
        "    batch_seg_map = sess.run(\n",
        "        OUTPUT_TENSOR_NAME,\n",
        "        feed_dict={INPUT_TENSOR_NAME: [np.asarray(resized_image)]})\n",
        "    seg_map = batch_seg_map[0]\n",
        "    return resized_image, seg_map\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "def getImage(img):\n",
        "  \"\"\"Inferences DeepLab model and visualizes result.\"\"\"\n",
        "  \n",
        "  f = open(str(SAMPLE_IMAGE.keys()[0]),'r')\n",
        "  jpeg_str = f.read()\n",
        "  original_im = Image.open(BytesIO(jpeg_str))\n",
        "  resized_im, seg_map = run(original_im)\n",
        "  return resized_im, seg_map\n",
        "\n",
        "resized_im, seg_map=getImage(SAMPLE_IMAGE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WNrkkX50FCF2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "blurImg = cv2.blur(np.asarray(resized_im),(10,10)) \n",
        "plt.imshow(blurImg)\n",
        "rMap=resized_im.load()\n",
        "portrait=blurImg\n",
        "pMap=blurImg\n",
        "cni=0\n",
        "cnj=0\n",
        "for i in range(resized_im.size[1]-1):\n",
        "    for j in range(resized_im.size[0]-1):\n",
        "      if not seg_map[i][j]==0:\n",
        "        cni=i\n",
        "        cnj=j\n",
        "        pMap[i,j]=rMap[j,i]\n",
        "\n",
        "\n",
        "cv2.imshow('image',pMap)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}